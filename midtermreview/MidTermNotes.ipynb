{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/rh/anaconda/root/envs/PUI2016_Python3/lib/python3.5/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from __future__ import division, print_function\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "import pylab as pl\n",
    "import seaborn\n",
    "%pylab inline\n",
    "\n",
    "np.seed = 999\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Science Buzzwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reproducibility: independent verification \n",
    "\n",
    "- Falsifiability: defining characteristic of science\n",
    "\n",
    "- Central Limit Theorem: The distribution of sample means is normally distributed around the true mean.\n",
    "\n",
    "### Types of data\n",
    "- qualitative: no ordering\n",
    "\n",
    "- quantitative: ordering is meaningful\n",
    "- - Continuous\n",
    "- - Discrete\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1](flow.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Errors\n",
    "\n",
    "## Statistical: \n",
    "\n",
    "Stochastic and random error\n",
    " - unpredictable uncertainty in a measurement due to lack of sensitivity\n",
    "\n",
    "- stochastic process can be completely random: Poisson process\n",
    "\n",
    "Solution: larger sample size\n",
    " \n",
    "\n",
    "## Systematic\n",
    "\n",
    "Tendency to underestimate/overestimate the average differnece between population and sample\n",
    "- Survery Bias\n",
    "- - Undercoverage Bias\n",
    "- - Self Selection Bias\n",
    "- - Social Desirability Bias\n",
    "- - Publication Bias\n",
    "Solution: Good experimental design, calibration, simulations\n",
    "\n",
    "\n",
    "## Reporting Errors\n",
    "- Add in quadrature (assuming Gaussian)\n",
    "\n",
    "## Type 1 vs Type 2\n",
    "- Type 1: False Positive -- Reject null when shouldn't\n",
    "- Type 2: False Negative -- Fail to reject null when should"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Munging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"myAPI\":\"XXXXXX-XXX-XXXX\"}"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'XXXXXX-XXX-XXXX'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!head apidef.json\n",
    "json_data = open(\"apidef.json\").read()\n",
    "myAPI = json.loads(json_data)\n",
    "myAPI[\"myAPI\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# reading data\n",
    "#pd.read_csv()\n",
    "\n",
    "#url = \"http://some.url.here\"\n",
    "#os.system(\"curl -O \" + url)\n",
    "#os.system(\"mv data.csv \" + os.getenv(\"PUIDATA\"))\n",
    "\n",
    "\n",
    "#url = (\"https://maps.googleapis.com/maps/api/geocode/json?latlng=\" +\n",
    "#          \"%f,%f&key=%s\"%(\n",
    "#            latlon[0], latlon[1], os.getenv('GOOGLEAPI')))\n",
    "\n",
    "#Never hard-code your API key in the code. Set an environmental variable \n",
    "\n",
    "#DFDATA = \"/gws/open/NYCOpenData/nycopendata/data/\"\n",
    "#df_gas = pd.read_csv(DFDATA + \"/uedp-fegm/1414245967/uedp-fegm\")\n",
    "\n",
    "#pd.drop(..., axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/fedhere/UInotebooks/blob/master/dataWrangling/PandasDataWrangling-Chap7.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal -- Gaussian"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For continuous variables, must have more than X samples, as stdev gets larger and variance gets larger, curve gets flatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1](gauss.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  74.34410066   51.42184615  108.38449537  158.13378052  140.85260894\n",
      "   58.62244899  125.29692014  130.87713101  108.82553256   53.63564495]\n"
     ]
    }
   ],
   "source": [
    "# change mu and sigma2\n",
    "# N(mu, sigma2) --> normally distributed with mean mu, variance sigma2 (sigma is stdev)\n",
    "# stdev is sqrt(variance)\n",
    "\n",
    "sigma = 25 # new standard deviation\n",
    "mu = 100\n",
    "\n",
    "g = sigma * np.random.randn(10) + mu\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Poisson: discrete variables, for counting, arrivals, pieces of mail, “queuing up”\n",
    "- independent events\n",
    "- lambda = mu = variance. Lambda is based on a historical average, very rarely given\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![1](poisson.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1](pd.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi Squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For continuous variables. With k degrees of freedom, is the sum of the squares of k independent standard normal random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1](chisq.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stating the Null Hypothesis:\n",
    "\n",
    "#### Verbally:\n",
    "\n",
    "Null Hypothesis: The mean of A is not different or is significantly greater than the mean of B.  \n",
    "Alternate Hypothesis: The mean of A is significantly less than the mean of B.\n",
    "\n",
    "#### Mathematically:\n",
    "\n",
    "$H_0$: A.mean() >= B.mean()\n",
    "\n",
    "$H_a$: A.mean() < B.mean()\n",
    "\n",
    "### $\\alpha=0.05$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z - Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# z = (mean_pop - mean_sample)/ (std_pop / sqrt(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T - Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# t = (mean_pop - mean_sample)/ (std_sample / sqrt(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test of difference of proportions\n",
    "\n",
    "# $z = \\frac{(p_0 - p_1)}{SE} $\n",
    "# $p =\\frac{p_0  n_0 + p_1  n_1}{n_0+n_1}$\n",
    "# $SE = \\sqrt{ p  ( 1 - p )  (\\frac{1}{n_0} + \\frac{1}{n_1}) }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pearson's Chi Squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independence\n",
    "\n",
    "Are unpaired observations of two variables independent?\n",
    "For a contingency table that has r rows and c columns, the chi square test can be thought of as a test of independence.  \n",
    "In a test of independence the null and alternative hypotheses are:  \n",
    "Ho: The two categorical variables are independent.  \n",
    "Ha: The two categorical variables are related.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1](chisq_i.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goodness of fit\n",
    "\n",
    "Does observed frequency differ from theoretical distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x2p = sum_i( (oi - ei)**2 / ei )\n",
    "# df = number of observations - num_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation chi-squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chisq = $\\sum_i \\frac{(model(x_i) - data(x_i))^2 }{ error_i^2}$  \n",
    "\n",
    "and $error_i$ is $\\sqrt{data(x_i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goodness of fit tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KS\n",
    "- Answers: are the samples likely to come from the same parent distribution\n",
    "- The Kolmogorov-Smirnov test (KS-test) tries to determine if two datasets differ significantly. The KS-test has the advantage of making no assumption about the distribution of data. (Technically speaking it is non-parametric and distribution free.)\n",
    "- - This test is used to decide if a sample comes from a hypothesized continuous distribution. \n",
    "- - It is based on the empirical cumulative distribution function (ECDF) \n",
    "- - Better for looking at the center of the data.\n",
    "- Hypotheses:\n",
    "- - Ho: that a sample is drawn from a population that follows a particular distribution\n",
    "- - Ha: The data do not follow the specified distribution.\n",
    "- - KS statistic (D) is based on the largest vertical difference between F(x) and Fn(x).\n",
    "- - Rejecting the Null: if p value is larger than (critical value for) confidence level, we cannot reject Ho.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1](ks.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AD\n",
    "\n",
    "- Answers: are the samples likely to come from the same parent distribution\n",
    "\n",
    "- The Anderson–Darling test is a statistical test of whether a given sample of data is drawn from a given probability distribution. In its basic form, the test assumes that there are no parameters to be estimated in the distribution being tested, in which case the test and its set of critical values is distribution-free.\n",
    "- - The AD assumes gaussian distribution test is a modification of the KS test.\n",
    "\n",
    "- - Better for looking at extremes.\n",
    "\n",
    "- - The AD procedure is a general test to compare the fit of an observed CDF to an expected cumulative distribution function (ECDF). This test gives more weight to the tails than the KS test.\n",
    "\n",
    "- Hypotheses\n",
    "- - Ho: the distributions are related (under implicit assumptions of gaussianity) / data follow specified distribution.\n",
    "- - Ha: The data follow the specified distribution.\n",
    "-  AD, on CDF, takes derivative at different points, and compares the slope, and that’s why it's more sensitive at the tails\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1](AD.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL Test\n",
    "The KL test is a non-symmetric measure of the difference between two probability distributions P and Q. Specifically, the KL divergendce of Q from P, denoted DKL(P‖Q), is a measure of the information lost when Q is used to approximate P. Typically P represents the \"true\" distribution of data, observations, or a precisely calculated theoretical distribution. The measure Q typically represents a theory, model, description, or approximation of P.\n",
    "- There is no Null for the KL divergence. \n",
    "- Can work on a PDF, doesn’t need a CDF\n",
    "\n",
    "![1](kl.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Used to compare if datasets are correlated, looks at paired values. Return a correlation coefficient. Note, correlation does not imply causation!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spearmans\n",
    "\n",
    "- Compares lineal distance of CDF’s at any point  \n",
    "- Looks at the center of the distribution  \n",
    "\n",
    "Returns 2-d array of correlation values\n",
    "Output (correlation coef, p value)\n",
    "Correlation coefficient: between -1 and 1. 0 implies no correlation. Correlations of -1 or +1 imply an “exact monotonic relationship”. Positive correlations imply that as x increases, so does y. Negative correlations imply that as x increases, y decreases.\n",
    "\n",
    "![1](spearman.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearsons test\n",
    "Compares derivative at different points of CDFs, which makes it more sensitive towards the tails as the slope is greater. The test is pairwise, so you have to sort your data so the pairs match.\n",
    "Returns: Output (correlation coef, p value)\n",
    "Correlation coefficient: between -1 and 1. 0 implies no correlation. Correlations of -1 or +1 imply an “exact monotonic relationship”. Positive correlations imply that as x increases, so does y. Negative correlations imply that as x increases, y decreases.\n",
    "\n",
    "![1](pearson.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood\n",
    "\n",
    "A likelihood function (often simply the likelihood) is a function of the parameters of a statistical model. Likelihood functions play a key role in statistical inference, especially methods of estimating a parameter from a set of statistics. In informal contexts, \"likelihood\" is often used as a synonym for \"probability.\" But in statistical usage, a distinction is made depending on the roles of the outcome or parameter. Probability is used when describing a function of the outcome given a fixed parameter value. For example, if a coin is flipped 10 times and it is a fair coin, what is the probability of it landing heads-up every time? Likelihood is used when describing a function of a parameter given an outcome. For example, if a coin is flipped 10 times and it has landed heads-up 10 times, what is the likelihood that the coin is fair?\n",
    "\n",
    "### Likelihood ratio test\n",
    "\n",
    "a statistical test used to compare the goodness of fit of two models, one of which (the null model) is a special case of the other (the alternative model). The test is based on the likelihood ratio, which expresses how many times more likely the data are under one model than the other. This likelihood ratio, or equivalently its logarithm, can then be used to compute a p-value, or compared to a critical value to decide whether to reject the null model in favour of the alternative model. When the logarithm of the likelihood ratio is used, the statistic is known as a log-likelihood ratio statistic.\n",
    "\n",
    "Being a function of the data , the likelihood ratio is therefore a statistic. The likelihood ratio test rejects the null hypothesis if the value of this statistic is too small. How small is too small depends on the significance level of the test, i.e., on what probability of Type I error is considered tolerable (\"Type I\" errors consist of the rejection of a null hypothesis that is true).\n",
    "The numerator corresponds to the maximum likelihood of an observed outcome under the null hypothesis. The denominator corresponds to the maximum likelihood of an observed outcome varying parameters over the whole parameter space. The numerator of this ratio is less than the denominator. The likelihood ratio hence is between 0 and 1. Low values of the likelihood ratio mean that the observed result was less likely to occur under the null hypothesis as compared to the alternative. High values of the statistic mean that the observed outcome was nearly as likely to occur under the null hypothesis as the alternative, and the null hypothesis cannot be rejected.\n",
    "\n",
    "![1](likelihood.PNG)\n",
    "\n",
    "### Likelihood Ratio Test\n",
    "The likelihood ratio test (LRT) is a statistical test of the goodness-of-fit between two models. A relatively more complex model is compared to a simpler model to see if it fits a particular dataset significantly better. If so, the additional parameters of the more complex model are often used in subsequent analyses. The LRT is only valid if used to compare hierarchically nested models. That is, the more complex model must differ from the simple model only by the addition of one or more parameters. Adding additional parameters will always result in a higher likelihood score. However, there comes a point when adding additional parameters is no longer justified in terms of significant improvement in fit of a model to a particular dataset. The LRT provides one objective criterion for selecting among possible models.\n",
    "The LRT begins with a comparison of the likelihood scores of the two models:\n",
    "LR = 2*(lnL1-lnL2)\n",
    "\n",
    "This LRT statistic approximately follows a chi-square distribution. To determine if the difference in likelihood scores among the two models is statistically significant, we next must consider the degrees of freedom. In the LRT, degrees of freedom is equal to the number of additional parameters in the more complex model. Using this information we can then determine the critical value of the test statistic from standard statistical tables.\n",
    "\n",
    "The likelihood range requires the difference in the degrees of freedom between the variables to be >=1 in order to compare its result with the chi square distribution\n",
    "\n",
    "You test fits of lines with a LR, and a higher LR indicates a better fit.\n",
    "\n",
    "### Nested models for line/curve fitting: \n",
    "Nested if one is a special case of another. \n",
    "For example: \n",
    "y = ax + b and \n",
    "y’ = ax2 + bx + c, where b=0\n",
    "are nested.\n",
    "Feeding a curve and feeding a line given the same independent variables is a nested model.\n",
    "Adding complexity always helps increase the fit of the model, and you should keep adding complexity until your model starts capturing noise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Standard Errors\n",
    "Standard errors of the estimate is a measure of the accuracy of predictions. Regression line minimizes the sum of squared deviations of prediction.\n",
    "\n",
    "![1](sterr.PNG)\n",
    "\n",
    "### Leverage\n",
    "The leverage of an observation is based on how much the observation's value on\n",
    "the predictor variable differs from the mean of the predictor variable. The greater\n",
    "an observation's leverage, the more potential it has to be an influential observation.\n",
    "\n",
    "### Regression towards the mean\n",
    "Regression toward the mean involves outcomes that are at least partly due to\n",
    "chance….This tendency of subjects with high values on a measure that includes chance and skill to score closer to the mean on a retest is called “regression toward the mean.”\n",
    "\n",
    "### Multiple Regression\n",
    "In multiple regression, the criterion is predicted by two or more variables. As in the case of simple linear regression, we define the best predictions as the predictions that minimize the squared errors of prediction.\n",
    "\n",
    "## Least square fits (OLS, WLS) → predictive models\n",
    "The method of least squares is a standard approach in regression analysis to the approximate solution of overdetermined systems, i.e., sets of equations in which there are more equations than unknowns. \"Least squares\" means that the overall solution minimizes the sum of the squares of the errors made in the results of every single equation.\n",
    "\n",
    "The most important application is in data fitting. The best fit in the least-squares sense minimizes the sum of squared residuals, a residual being the difference between an observed value and the fitted value provided by a model. \n",
    "\n",
    "Least squares problems fall into two categories: linear or ordinary/linear least squares and non-linear least squares, depending on whether or not the residuals are linear in all unknowns. The ordinary/ linear least-squares problem occurs in statistical regression analysis; it has a closed-form solution. \n",
    "\n",
    "### Ordinary least squares (OLS)\n",
    "or linear least squares is a method for estimating the unknown parameters in a linear regression model, with the goal of minimizing the differences between the observed responses in some arbitrary dataset and the responses predicted by the linear approximation of the data (visually this is seen as the sum of the vertical distances between each data point in the set and the corresponding point on the regression line - the smaller the differences, the better the model fits the data). The resulting estimator can be expressed by a simple formula\n",
    "\n",
    "### Weighted least squares\n",
    "A special case of generalized least squares called weighted least squares occurs when all the off-diagonal entries of Ω (the correlation matrix of the residuals) are null; the variances of the observations (along the covariance matrix diagonal) may still be unequal (heteroskedasticity).\n",
    "The expressions given above are based on the implicit assumption that the errors are uncorrelated with each other and with the independent variables and have equal variance. \n",
    "\n",
    "### Curve Fitting\n",
    "Curve fitting is the process of constructing a curve, or mathematical function, that has the best fit to a series of data points, possibly subject to constraints... A related topic is regression analysis, which focuses more on questions of statistical inference such as how much uncertainty is present in a curve that is fit to data observed with random errors. Fitted curves can be used as an aid for data visualization, to infer values of a function where no data are available, and to summarize the relationships among two or more variables. \n",
    "Can be done with a polynomial curve. \n",
    "If the order of the equation is increased to a second degree polynomial, the following results:\n",
    "y = ax**2 + bx + c\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PUI2016_Python3",
   "language": "python",
   "name": "pui2016_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
